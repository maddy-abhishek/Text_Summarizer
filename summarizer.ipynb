{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install transformers sentencepiece PyPDF2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\r\n",
    "import torch\r\n",
    "import PyPDF2\r\n",
    "from google.colab import files"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "uploaded = files.upload()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def extract_text_from_pdf(file):\r\n",
    "    reader = PyPDF2.PdfReader(file)\r\n",
    "    text = \"\"\r\n",
    "    for page in reader.pages:\r\n",
    "        text += page.extract_text()\r\n",
    "    return text\r\n",
    "\r\n",
    "# Get filename from uploaded dict\r\n",
    "pdf_filename = list(uploaded.keys())[0]\r\n",
    "text = extract_text_from_pdf(pdf_filename)\r\n",
    "\r\n",
    "print(\"âœ… PDF uploaded and text extracted.\")\r\n",
    "print(\"ðŸ“„ Preview of extracted text:\\n\")\r\n",
    "print(text[:1000])  # preview only"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_name = \"t5-small\"\r\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\r\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\r\n",
    "\r\n",
    "# Use GPU if available\r\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
    "model = model.to(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def summarize_text(text, max_len=150, min_len=70):\r\n",
    "    # Split if too long\r\n",
    "    if len(text) > 1000:\r\n",
    "        text = text[:1000]  # truncate long docs for demo\r\n",
    "\r\n",
    "    preprocessed = \"summarize: \" + text.strip().replace(\"\\n\", \" \")\r\n",
    "    inputs = tokenizer.encode(preprocessed, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\r\n",
    "\r\n",
    "    summary_ids = model.generate(inputs, max_length=max_len, min_length=min_len,\r\n",
    "                                 num_beams=4, length_penalty=2.0, early_stopping=True)\r\n",
    "\r\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\r\n",
    "    return summary"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "summary = summarize_text(text)\r\n",
    "print(\"\\n Summary:\\n\")\r\n",
    "print(summary)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}